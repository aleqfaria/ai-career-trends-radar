# AI Career Trends Radar ğŸ“ˆğŸ¤–

An end-to-end data pipeline to monitor and analyze emerging career trends in Artificial Intelligence (AI) and technology fields â€” including Data Engineering, AI Engineering, and Cybersecurity.

## ğŸ¯ Objective

To build a fully automated ETL pipeline that:

- Collects career trend data from sources like **Google Trends** and **OpenAI (ChatGPT)**
- Transforms raw data into clean, structured insights
- Generates weekly reports in **CSV/JSON**
- Prepares data for dashboards and long-term analysis
- Is extensible to include new sources like **Reddit**, **LinkedIn**, or **Kaggle**

## ğŸ› ï¸ Technologies Used

- **Python** â€“ scripting, automation
- **Pandas** â€“ data manipulation
- **OpenAI API** â€“ career insights generation
- **Google Trends (PyTrends)** â€“ popularity over time
- **Apache Airflow** â€“ pipeline automation (coming soon)
- **Git & GitHub** â€“ version control
- **Docker** â€“ environment reproducibility
- **SQLite** â€“ optional local storage

## ğŸ“ Project Structure

```
ai-career-trends-radar/
â”œâ”€â”€ etl/             # extract.py, transform.py, load.py scripts
â”œâ”€â”€ data/            # raw and processed data (CSV, JSON, Markdown)
â”œâ”€â”€ reports/         # auto-generated weekly summaries
â”œâ”€â”€ scripts/         # automation helpers (scheduling, email, etc.)
â”œâ”€â”€ .env.example     # template for your OpenAI API key
â”œâ”€â”€ main.py          # entry point for pipeline execution
â””â”€â”€ README.md
```

## ğŸ”„ Workflow

1. **Extract** data from:
   - Google Trends (job title search volume)
   - OpenAI (insights on emerging roles via GPT)
2. **Transform** raw data into structured DataFrames
3. **Load** into CSV and JSON formats for further analysis
4. (Soon) **Automate** the entire process using Apache Airflow

## ğŸš€ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/seu-usuario/ai-career-trends-radar.git
cd ai-career-trends-radar
```

### 2. Set up the virtual environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Configure API key
Create a `.env` file:
```env
OPENAI_API_KEY=sk-xxxxxxx...
```

### 4. Run the ETL pipeline manually
```bash
python main.py
```

---

## ğŸ“ Status

ğŸš§ **Project in active development**  
Upcoming features:
- Airflow DAG integration
- Streamlit dashboard
- Multi-language support (EN/PT)

---

## ğŸ¤ Contributing

This is a personal learning project but open to improvements.  
Feel free to fork, suggest enhancements, or create pull requests.

---

## ğŸ“¬ Contact

Created by [Alexandre Faria](https://www.linkedin.com/in/aleqfaria) â€“ transitioning into Data Engineering.
